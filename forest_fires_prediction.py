# -*- coding: utf-8 -*-
"""FOREST_FIRES_PREDICTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d-SasiPd_aHDXXlX8iaoyvojLDnoah6y
"""

# Import library for exploring dataset
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("forest_fires(2).csv")
df.replace(['?', '/', '#'], np.nan, inplace=True)

df.head()

df.shape

df.columns

df.describe()

df.isnull().sum()

df['day'].value_counts()

# Calculate the mode
mode_val = df['day'].mode()[0]

# Fill the null values
df['day'].fillna(mode_val, inplace=True)

df['DMC'].value_counts()

# Calculate the mode
median_val = df['DMC'].median()

# Fill the null values
df['DMC'].fillna(median_val, inplace=True)

df['temp'].value_counts()

# Calculate the mode
median_val = df['temp'].median()

# Fill the null values
df['temp'].fillna(median_val, inplace=True)

df['wind'].value_counts()

# Calculate the mode
median_val = df['wind'].median()

# Fill the null values
df['wind'].fillna(median_val, inplace=True)

df.isnull().any()

df.info()

df.describe()

# describe categorical features

df.describe(include="O")

num_cols = df.select_dtypes(include='number').columns

fig, axs =  plt.subplots(nrows=5, ncols=2, figsize=(15,20))
axs = np.ravel(axs)

for i, col in enumerate(num_cols[2:]):
    plt.sca(axs[i])
    sns.histplot(data=df, x=col, kde=True, line_kws={'linewidth':2, 'linestyle':'--'}, color='orange')
    
plt.tight_layout()
plt.show()

df.month.value_counts()

df.day.value_counts()

# setting parameters
plt.rcParams['figure.figsize'] = [20, 10]
sns.set(style = "darkgrid", font_scale = 1.3)
month_temp = sns.barplot(x = 'month', y = 'temp', data = df,
                         order = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'], palette = 'winter');
month_temp.set(title = "Month Vs Temp Barplot", xlabel = "Months", ylabel = "Temperature");

fig, ax = plt.subplots(figsize=(6,6))
sns.set_style('whitegrid')
sns.countplot(x='day', data=df)

fig, ax = plt.subplots(figsize=(6,6))
sns.set_style('whitegrid')
sns.scatterplot(data=df ,x='temp', y='area', color='b')

## Data mapping of categorical features

month_map = {'jan':1, 'feb':2, 'mar':3, 
             'apr':4, 'may':5, 'jun':6, 
             'jul':7, 'aug':8, 'sep':9, 
             'oct':10, 'nov':11, 'dec':12}

day_map = {'mon':1, 'tue':2, 'wed':3,
          'thu':4, 'fri':5, 'sat':6, 'sun':7}

df.month = df.month.map(month_map)
df.day = df.day.map(day_map)

# Correlation Heatmap of the features in the dataset
plt.rcParams['figure.figsize'] = [12, 10]
sns.set(font_scale = 1)
sns.heatmap(df.corr(), annot = True);

df.columns

#checking outiliers in dataset
fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))
index = 0
axs = axs.flatten()
for k,v in df.items():
    sns.boxplot(y=k, data= df, ax=axs[index])
    index += 1
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)

# Identify the columns with potential outliers
outlier_cols = ['X', 'Y', 'month', 'day', 'FFMC', 'DMC', 'DC', 'ISI', 'temp',
       'RH', 'wind', 'rain','area']

# Replace outliers with the upper and lower bounds
for col in outlier_cols:
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    upper_bound = q3 + 1.5*iqr
    lower_bound = q1 - 1.5*iqr
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])

#checking outiliers in dataset
fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))
index = 0
axs = axs.flatten()
for k,v in df.items():
    sns.boxplot(y=k, data= df, ax=axs[index])
    index += 1
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)

df.head()

df.info()

df=df.drop('id',axis=1)

import numpy as np

class DecisionTreeRegressor:
    def __init__(self, max_depth=None, min_samples_split=2):
        self.max_depth = max_depth#determines the maximum depth of the decision tree that will be constructed
        self.min_samples_split = min_samples_split#specifies the minimum number of samples required to split an internal node
        self.tree = None

    def mean_squared_error(self, y):
        return np.mean((y - np.mean(y)) ** 2)# Calculate mean squared error of targets

    def split_data(self, X, y, feature_idx, threshold):
        left_mask = X[:, feature_idx] <= threshold# find the indices of samples where the feature is less than the threshold value
        right_mask = X[:, feature_idx] > threshold # find the indices of samples where the feature is greater than or equal to the split value
        X_left, y_left = X[left_mask], y[left_mask] #create a new array of input samples for the left node using the left indices, #create a new array of labels for the left node using the left indice
        X_right, y_right = X[right_mask], y[right_mask]# create a new array of input samples for the right node using the right indices,# create a new array of labels for the right node using the right indices
        return X_left, y_left, X_right, y_right# return the new input and label arrays for the left and right nodes

    def find_best_split(self, X, y):
        ''' function to find the best split '''
        best_feature_idx, best_threshold, best_mse = None, None, np.inf
         #ensure that the first value you encounter will be greater than the current maximum value
         # loop over all the features in the dataset
        for feature_idx in range(X.shape[1]):
            # loop over all the unique feature values present in the data
            for threshold in np.unique(X[:, feature_idx]):
              # get current split
                X_left, y_left, X_right, y_right = self.split_data(X, y, feature_idx, threshold)
                #The code checks if the number of samples in the left and right subsets of the current dataset are less than a specified minimum threshold 
                if len(y_left) < self.min_samples_split or len(y_right) < self.min_samples_split:
                    continue
               # The split separates the data into two subsets: one on the left and the other on the right. The mean squared error is calculated for each subset separately using the method mean_squared_error()
                mse_left, mse_right = self.mean_squared_error(y_left), self.mean_squared_error(y_right)
                mse = mse_left + mse_right
                #It compares the MSE for each split to the current best MSE and updates the best feature index, best threshold, and best MSE if the current split has a lower MSE than the current best split.
                if mse < best_mse:
                    best_feature_idx, best_threshold, best_mse = feature_idx, threshold, mse
        return best_feature_idx, best_threshold, best_mse

    def build_tree(self, X, y, depth):
        if depth == self.max_depth or len(y) < self.min_samples_split:
            return np.mean(y)
        feature_idx, threshold, mse = self.find_best_split(X, y)
        if mse == np.inf:
            return np.mean(y)
        X_left, y_left, X_right, y_right = self.split_data(X, y, feature_idx, threshold)
        left_node = self.build_tree(X_left, y_left, depth + 1)
        right_node = self.build_tree(X_right, y_right, depth + 1)
        return {"feature_idx": feature_idx, "threshold": threshold, "left_node": left_node, "right_node": right_node}

    def fit(self, X, y):
        self.tree = self.build_tree(X, y, 0)
    def set_params(self, **params):
          '''function is used to set the values of the attributes of a decision tree object. The function takes a variable 
          number of keyword arguments (**params), 
          which are pairs of attribute names and their corresponding values that should be set for the decision tree object'''
          for param, value in params.items():
            setattr(self, param, value)
            return self

    def predict(self, X):
        def predict_row(row, node):
            if isinstance(node, float):
                return node
            if row[node["feature_idx"]] <= node["threshold"]:
                return predict_row(row, node["left_node"])
            else:
                return predict_row(row, node["right_node"])
        return np.array([predict_row(row, self.tree) for row in X])
    def mean_squared_errorr(self,y_true, y_pred):
   
      # Check if the lengths of both arrays are equal
      if len(y_true) != len(y_pred):
          raise ValueError("Length of y_true and y_pred should be the same.")
      
      # Calculate the squared differences between the true and predicted values
      squared_differences = [(y_true[i] - y_pred[i])**2 for i in range(len(y_true))]
      
      # Calculate the mean of the squared differences
      mse = sum(squared_differences) / len(squared_differences)
      
      return mse

# Scale the numerical features using min-max scaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df[['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']] = scaler.fit_transform(df[['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']])

from sklearn.model_selection import train_test_split

X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

regressor = DecisionTreeRegressor(min_samples_split=4, max_depth=4)
regressor.fit(X_train,y_train)

y_pred = regressor.predict(X_test)

import numpy as np
def mean_squared_error(y_true, y_pred):
    """
    Calculates the mean squared error between y_true and y_pred.
    :param y_true: A list or array of true values
    :param y_pred: A list or array of predicted values
    :return: The mean squared error between y_true and y_pred
    """
    # Get the length of the arrays
    n = len(y_true)
    
    # Calculate the sum of squared differences
    sum_squared_diff = sum((y_true[i] - y_pred[i])**2 for i in range(n))
    
    # Calculate the mean squared error
    mse = sum_squared_diff / n
    
    return mse

mse = mean_squared_error(y_test, y_pred)
mse



df.head()

### Create a Pickle file using serialization 
import pickle
pickle_out = open("regressor.pkl","wb")
pickle.dump(regressor, pickle_out)
pickle_out.close()

df.columns

regressor.predict([[7.0,5.0,4.0,5.0,0.004,5,2,5,12,2,22,32]])

